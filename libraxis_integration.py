import requests
import json
import asyncio
from datetime import datetime
from typing import Optional, Dict, Any

class LibraxisAIClient:
    """Client dla integracji z waszymi endpointami LibraxisAI"""
    
    def __init__(self, base_url: str = "https://llm.libraxis.cloud/api/v1"):
        self.base_url = base_url
        self.headers = {
            "Content-Type": "application/json",
            # Dodajcie API key jeÅ›li uÅ¼ywacie
            # "Authorization": "Bearer YOUR_API_KEY"
        }
    
    async def get_available_models(self) -> list:
        """Pobierz listÄ™ dostÄ™pnych modeli"""
        try:
            response = requests.get(f"{self.base_url}/models", headers=self.headers)
            if response.status_code == 200:
                return response.json()
            return []
        except Exception as e:
            print(f"BÅ‚Ä…d pobierania modeli: {e}")
            return []
    
    async def generate_response(self, message: str, model: str = None, context: list = None) -> str:
        """Wygeneruj odpowiedÅº uÅ¼ywajÄ…c waszego LLM API"""
        try:
            # Adaptuj format do waszego API
            payload = {
                "model": model or "default",  # UÅ¼yjcie domyÅ›lnego modelu
                "messages": context or [{"role": "user", "content": message}],
                "max_tokens": 150,
                "temperature": 0.7
            }
            
            response = requests.post(
                f"{self.base_url}/chat/completions",  # SprawdÅºcie wÅ‚aÅ›ciwy endpoint
                headers=self.headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                # Adaptuj do formatu odpowiedzi z waszego API
                return result.get("choices", [{}])[0].get("message", {}).get("content", "")
            else:
                return f"BÅ‚Ä…d API: {response.status_code}"
                
        except Exception as e:
            print(f"BÅ‚Ä…d generowania odpowiedzi: {e}")
            return "Przepraszam, mam problem z poÅ‚Ä…czeniem."

class EnhancedTeamChatClient:
    """Rozszerzony client chatu z integracjÄ… LibraxisAI"""
    
    def __init__(self, chat_server_url: str = "http://localhost:8000"):
        self.chat_url = chat_server_url
        self.sender_name = "Claude-LibraxisAI"
        self.last_message_timestamp = None
        self.libraxis_client = LibraxisAIClient()
        self.conversation_context = []  # Historia konwersacji dla kontekstu
        
    def send_message(self, content: str) -> bool:
        """WyÅ›lij wiadomoÅ›Ä‡ do team chatu"""
        try:
            response = requests.post(
                f"{self.chat_url}/api/send",
                json={
                    "sender": self.sender_name,
                    "content": content
                },
                timeout=10
            )
            return response.status_code == 200
        except Exception as e:
            print(f"BÅ‚Ä…d wysyÅ‚ania wiadomoÅ›ci: {e}")
            return False
    
    def get_new_messages(self) -> list:
        """Pobierz nowe wiadomoÅ›ci od ostatniego sprawdzenia"""
        try:
            url = f"{self.chat_url}/api/messages"
            if self.last_message_timestamp:
                url += f"?since={self.last_message_timestamp}"
            
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                messages = response.json()
                # Filtruj wÅ‚asne wiadomoÅ›ci
                new_messages = [m for m in messages if m["sender"] != self.sender_name]
                
                if new_messages:
                    self.last_message_timestamp = max(m["timestamp"] for m in messages)
                
                return new_messages
            return []
            
        except Exception as e:
            print(f"BÅ‚Ä…d pobierania wiadomoÅ›ci: {e}")
            return []
    
    def update_conversation_context(self, message: dict):
        """Aktualizuj kontekst konwersacji"""
        self.conversation_context.append({
            "role": "user",
            "content": f"{message['sender']}: {message['content']}"
        })
        
        # Zachowaj tylko ostatnie 10 wiadomoÅ›ci dla kontekstu
        if len(self.conversation_context) > 10:
            self.conversation_context = self.conversation_context[-10:]
    
    async def generate_smart_response(self, message: dict) -> Optional[str]:
        """Wygeneruj inteligentnÄ… odpowiedÅº uÅ¼ywajÄ…c LibraxisAI"""
        content = message["content"].lower()
        sender = message["sender"]
        
        # Dodaj wiadomoÅ›Ä‡ do kontekstu
        self.update_conversation_context(message)
        
        # SprawdÅº czy Claude powinien odpowiedzieÄ‡
        should_respond = any([
            "claude" in content,
            "?" in content and len(content.split()) < 15,  # KrÃ³tkie pytania
            "code review" in content,
            "bug" in content or "bÅ‚Ä…d" in content,
            "deploy" in content,
            "help" in content or "pomoc" in content,
            "status" in content
        ])
        
        if not should_respond:
            return None
        
        # Przygotuj prompt z kontekstem
        system_prompt = f"""JesteÅ› Claude Code, czÅ‚onek zespoÅ‚u 3 humans + 3 AI. 
        Pomagasz w:
        - Code review i analiza kodu
        - Monitorowanie repo VISTY 
        - Debugging i rozwiÄ…zywanie problemÃ³w
        - Deployment i DevOps
        
        Odpowiadaj krÃ³tko, konkretnie i pomocnie. UÅ¼ywaj emoji tam gdzie pasuje.
        Zwracaj siÄ™ do uÅ¼ytkownika po imieniu: @{sender}"""
        
        # Przygotuj peÅ‚ny kontekst dla LLM
        full_context = [
            {"role": "system", "content": system_prompt},
            *self.conversation_context[-5:],  # Ostatnie 5 wiadomoÅ›ci kontekstu
        ]
        
        try:
            response = await self.libraxis_client.generate_response(
                message=content,
                context=full_context
            )
            
            return response if response and len(response.strip()) > 0 else None
            
        except Exception as e:
            print(f"BÅ‚Ä…d generowania smart response: {e}")
            # Fallback do prostych odpowiedzi
            return self.get_fallback_response(content, sender)
    
    def get_fallback_response(self, content: str, sender: str) -> Optional[str]:
        """Proste odpowiedzi fallback gdy LibraxisAI nie dziaÅ‚a"""
        if "code review" in content:
            return f"@{sender} Sprawdzam kod! ğŸ” Daj mi chwilÄ™ na analizÄ™."
        elif "deploy" in content:
            return f"@{sender} MonitorujÄ™ deployment... ğŸš€"
        elif "bug" in content or "bÅ‚Ä…d" in content:
            return f"@{sender} AnalizujÄ™ problem ğŸ› KtÃ³re pliki sÄ… dotkniÄ™te?"
        elif "status" in content:
            return "âœ… Claude Code aktywny | ğŸ” Monitoring VISTY | ğŸ’¬ LibraxisAI connected"
        elif "claude" in content and "?" in content:
            return f"@{sender} Jestem tu! W czym mogÄ™ pomÃ³c? ğŸ¤–"
        return None
    
    async def start_monitoring(self):
        """Rozpocznij monitoring z AI-powered responses"""
        print("ğŸ¤– Claude Code + LibraxisAI - monitoring team chat...")
        
        # SprawdÅº dostÄ™pne modele przy starcie
        models = await self.libraxis_client.get_available_models()
        if models:
            print(f"ğŸ“Š DostÄ™pne modele LibraxisAI: {len(models)}")
            self.send_message(f"ğŸ¤– Claude Code poÅ‚Ä…czony! Using LibraxisAI models: {len(models)} dostÄ™pnych ğŸ’ª")
        else:
            print("âš ï¸ Nie udaÅ‚o siÄ™ pobraÄ‡ listy modeli, uÅ¼ywam fallback responses")
            self.send_message("ğŸ¤– Claude Code poÅ‚Ä…czony! (Fallback mode) ğŸ’ª")
        
        while True:
            try:
                new_messages = self.get_new_messages()
                
                for message in new_messages:
                    print(f"ğŸ“¨ Nowa wiadomoÅ›Ä‡ od {message['sender']}: {message['content']}")
                    
                    # Wygeneruj smart response
                    response = await self.generate_smart_response(message)
                    
                    if response:
                        self.send_message(response)
                        print(f"âœ… OdpowiedziaÅ‚em: {response}")
                
                await asyncio.sleep(5)  # Sprawdzaj co 5 sekund
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Claude Code - koÅ„czy monitoring")
                break
            except Exception as e:
                print(f"BÅ‚Ä…d w monitoringu: {e}")
                await asyncio.sleep(10)

# GÅ‚Ã³wna funkcja z integracjÄ… LibraxisAI
async def main():
    chat_client = EnhancedTeamChatClient()
    await chat_client.start_monitoring()

if __name__ == "__main__":
    asyncio.run(main())
